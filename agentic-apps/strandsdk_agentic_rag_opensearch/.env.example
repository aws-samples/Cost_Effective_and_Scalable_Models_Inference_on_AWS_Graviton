# LiteLLM Configuration for Reasoning Models
LITELLM_API_KEY=your-litellm-api-key
LITELLM_BASE_URL=http://your-litellm-server:8080/v1
REASONING_MODEL=qwen-qwq-32b-preview

# Embedding Configuration (separate from reasoning)
EMBEDDING_API_KEY=your-embedding-api-key
EMBEDDING_BASE_URL=http://your-embedding-server:8080/v1
EMBEDDING_MODEL=llamacpp-embedding

# Legacy OpenAI Configuration (for backward compatibility)
OPENAI_API_KEY=your-openai-api-key
OPENAI_BASE_URL=https://api.openai.com/v1
DEFAULT_MODEL=qwen-qwq-32b-preview

# AWS Configuration  
AWS_REGION=us-east-1
OPENSEARCH_ENDPOINT=https://your-opensearch-domain.region.es.amazonaws.com

# Tavily Web Search Configuration
TAVILY_API_KEY=your-tavily-api-key

# Optional: Langfuse for observability
LANGFUSE_HOST=https://cloud.langfuse.com
LANGFUSE_PUBLIC_KEY=your-public-key
LANGFUSE_SECRET_KEY=your-secret-key

# Application Settings
KNOWLEDGE_DIR=knowledge
OUTPUT_DIR=output
VECTOR_INDEX_NAME=knowledge-embeddings
TOP_K_RESULTS=5
BYPASS_TOOL_CONSENT=true

# Configuration Notes:
# 
# LITELLM_API_KEY: API key for your LiteLLM server hosting reasoning models
# LITELLM_BASE_URL: Endpoint for LiteLLM server (e.g., http://localhost:8080/v1)
# REASONING_MODEL: Model name for agent reasoning tasks (e.g., qwen-qwq-32b-preview)
# 
# EMBEDDING_API_KEY: API key for embedding service (can be same as LITELLM_API_KEY)
# EMBEDDING_BASE_URL: Endpoint for embedding generation (can be same as LITELLM_BASE_URL)
# EMBEDDING_MODEL: Model name for generating embeddings (e.g., llamacpp-embedding)
# 
# OPENAI_*: Legacy configuration for backward compatibility
# DEFAULT_MODEL: Fallback model ID if LiteLLM fails
# 
# AWS_REGION: AWS region for OpenSearch and other AWS services
# OPENSEARCH_ENDPOINT: Your AWS OpenSearch domain endpoint
# 
# TAVILY_API_KEY: API key for Tavily web search service (get from https://tavily.com)
# 
# LANGFUSE_*: Optional observability tracking (leave empty to disable)
# 
# KNOWLEDGE_DIR: Directory containing knowledge files to embed
# OUTPUT_DIR: Directory for generated outputs and reports
# VECTOR_INDEX_NAME: OpenSearch index name for vector storage
# TOP_K_RESULTS: Default number of search results to return
#
# Model Usage:
# - Reasoning Tasks (All Agents): Uses REASONING_MODEL via LiteLLM
# - Embedding Tasks (RAG): Uses EMBEDDING_MODEL via embedding endpoint
# - Web Search: Uses Tavily API for real-time information when RAG relevance is low
# - Both LLM models can point to the same LiteLLM server with different model names
