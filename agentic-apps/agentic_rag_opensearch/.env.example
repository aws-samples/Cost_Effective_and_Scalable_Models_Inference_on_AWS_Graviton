# OpenAI API Configuration
# API key for accessing the LLM model
# This should match the key configured in your LiteLLM gateway
OPENAI_API_KEY=your-openai-api-key

# Base URL for the OpenAI-compatible API
# This should be the URL of your model hosting service (Ray service or LiteLLM gateway)
OPENAI_BASE_URL=http://your-model-endpoint/v1

# OpenSearch Configuration
# Endpoint URL of your OpenSearch cluster
# This will be automatically set by the setup-opensearch.sh script
OPENSEARCH_ENDPOINT=https://your-opensearch-domain-endpoint

# AWS region where your OpenSearch cluster is deployed
AWS_REGION=us-east-1

# Langfuse Configuration for LLM Observability
# URL of the Langfuse service deployed in your EKS cluster
# This should be the load balancer URL from your model-observability deployment
LANGFUSE_HOST=http://your-langfuse-host-url

# Langfuse public key for authentication
# Get this from your Langfuse dashboard after creating a project
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key

# Langfuse secret key for authentication
# Get this from your Langfuse dashboard after creating a project
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key
