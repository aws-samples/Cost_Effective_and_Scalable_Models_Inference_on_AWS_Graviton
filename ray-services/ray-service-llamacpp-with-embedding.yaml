apiVersion: v1
kind: Secret
metadata:
  name: token-embedding
stringData:
  token: HUGGING_FACE_HUB_TOKEN
  llm_api_key: sk-1234
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-app-code-embedding
data:
  app.py: |
    import multiprocessing
    import os
    import logging
    import time
    import json
    import numpy as np
    from fastapi import FastAPI, HTTPException
    from starlette.requests import Request
    from starlette.responses import JSONResponse
    from ray import serve
    from llama_cpp import Llama

    logger = logging.getLogger("ray.serve")
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    app = FastAPI()

    @serve.deployment(
        name="EmbeddingDeployment", 
        ray_actor_options={"num_cpus": 29}, 
        autoscaling_config={
            "min_replicas": 1, 
            "max_replicas": 10, 
            "initial_replicas": 1, 
            "upscale_delay_s": 5
        }, 
        max_ongoing_requests=100, 
        graceful_shutdown_timeout_s=600
    )
    @serve.ingress(app)
    class EmbeddingDeployment:
        def __init__(self, parallelism: str):
            os.environ["OMP_NUM_THREADS"] = parallelism
            
            # Embedding model configuration
            self.embedding_model_id = os.getenv("EMBEDDING_MODEL_ID", default="ddynwzh1992/embedding")
            self.embedding_filename = os.getenv("EMBEDDING_MODEL_FILENAME", default="Snowflake-Arctic-Embed-s_FT-33M-F16.gguf")
            self.n_ctx = int(os.getenv("N_CTX", default="0"))
            self.n_threads = int(os.getenv("N_THREADS", default="32"))
            self.api_key = os.getenv("LLM_API_KEY", default="sk-1234")
            
            # Load embedding model
            self.embedding_model = Llama.from_pretrained(
                repo_id=self.embedding_model_id,
                filename=self.embedding_filename,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads,
                embedding=True
            )
            logger.info(f"Embedding model {self.embedding_model_id} loaded successfully with {self.n_threads} threads")
            
            print("__init__ Complete")

        @app.post("/v1/embeddings")
        async def create_embedding(self, request: Request):
            try:
                start_time = time.time()
                body = await request.json()
                
                # Check for API key in request headers
                auth_header = request.headers.get("Authorization", "")
                if auth_header.startswith("Bearer "):
                    api_key = auth_header.replace("Bearer ", "")
                    if api_key != self.api_key:
                        return JSONResponse(
                            status_code=401,
                            content={"error": "Invalid API key"}
                        )
                
                # Extract parameters from request
                input_texts = body.get("input", [])
                if isinstance(input_texts, str):
                    input_texts = [input_texts]
                
                model = body.get("model", self.embedding_model_id)
                
                # Log request info
                logger.info(f"Received embedding request for {len(input_texts)} texts")
                
                # Generate embeddings
                embeddings = []
                tokens = 0
                
                for text in input_texts:
                    # Generate embedding
                    embedding_output = self.embedding_model.embed(text)
                    
                    # Convert to list
                    embedding_vector = embedding_output.tolist()
                    
                    # Add to results
                    embeddings.append({
                        "object": "embedding",
                        "embedding": embedding_vector,
                        "index": len(embeddings)
                    })
                    
                    # Count tokens
                    tokens += len(text.split())
                
                # Prepare the response
                response = {
                    "object": "list",
                    "data": embeddings,
                    "model": model,
                    "usage": {
                        "prompt_tokens": tokens,
                        "total_tokens": tokens
                    }
                }
                
                duration = time.time() - start_time
                logger.info(f"Embedding request processed in {duration:.2f} seconds")
                
                return JSONResponse(content=response)
                
            except Exception as e:
                logger.error(f"Error processing embedding request: {str(e)}")
                return JSONResponse(
                    status_code=500,
                    content={"error": str(e)}
                )

        @app.get("/health")
        async def health_check(self):
            return {"status": "healthy", "model": self.embedding_model_id}

    host_cpu_count = multiprocessing.cpu_count()

    model = EmbeddingDeployment.bind("host_cpu_count")
---
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: ray-service-llamacpp-with-embedding
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300
  serveConfigV2: |
    applications:
      - name: embedding
        import_path: app.model
        runtime_env:
          working_dir: /home/ray
          pip:
            - llama-cpp-python==0.2.56
            - fastapi==0.110.0
            - uvicorn==0.27.1
            - starlette==0.37.2
            - numpy==1.26.4
  rayClusterConfig:
    headGroupSpec:
      rayStartParams:
        dashboard-host: 0.0.0.0
        num-cpus: "0"
      template:
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                    - arm64           
          containers:
          - name: ray-head
            image: rayproject/ray:2.39.0-py312-cpu-aarch64
            ports:
            - containerPort: 6379
              name: gcs
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            resources:
              limits:
                cpu: "1"
                memory: "2Gi"
              requests:
                cpu: "1"
                memory: "2Gi"
    workerGroupSpecs:
      - replicas: 1
        minReplicas: 1
        maxReplicas: 10
        groupName: embedding-worker
        rayStartParams:
          num-cpus: "30"
        template:
          spec:
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/arch
                      operator: In
                      values:
                      - arm64           
            restartPolicy: Always
            containers:
            - name: llm
              image: rayproject/ray:2.39.0-py312-cpu-aarch64
              volumeMounts:
                - name: app-code
                  mountPath: /home/ray/app.py
                  subPath: app.py           
              env:
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: token-embedding
                    key: token
              - name: LLM_API_KEY
                valueFrom:
                  secretKeyRef:
                    name: token-embedding
                    key: llm_api_key
              - name: CMAKE_ARGS
                value: "-DCMAKE_CXX_FLAGS=-fopenmp"  
              - name: "CC"
                value: "/usr/bin/gcc"   
              - name: "CXX"
                value: "/usr/bin/g++"   
              - name: CMAKE_CXX_COMPILER
                value: "/usr/bin/g++"  
              - name: CMAKE_C_COMPILER
                value: "/usr/bin/gcc" 
              - name: PYTHONPATH
                value: "/home/ray/anaconda3/lib/python3.11/site-packages:$PYTHONPATH" 
              - name: EMBEDDING_MODEL_ID
                value: "ddynwzh1992/embedding"
              - name: EMBEDDING_MODEL_FILENAME
                value: "Snowflake-Arctic-Embed-s_FT-33M-F16.gguf"
              - name: N_CTX
                value: "0"
              - name: N_THREADS
                value: "32"
              resources:
                limits:
                  cpu: "30"
                  memory: "55Gi"        
                requests:
                  cpu: "30"
                  memory: "55Gi"
            volumes:
                - name: app-code
                  configMap:
                    name: llama-app-code-embedding
