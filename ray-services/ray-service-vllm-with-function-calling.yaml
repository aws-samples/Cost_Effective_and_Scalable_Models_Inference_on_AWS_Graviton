---
apiVersion: v1
kind: Secret
metadata:
  name: hf-token
stringData:
  llm-api-key: sk-1234
  hf-token: HUGGING_FACE_HUB_TOKEN
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-code
data:
  vllm.py: |
    import json
    from typing import AsyncGenerator, Dict, List, Optional, Union, Any
    from fastapi import BackgroundTasks
    from starlette.requests import Request
    from starlette.responses import StreamingResponse, Response, JSONResponse
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    from vllm.sampling_params import SamplingParams
    from vllm.utils import random_uuid
    from ray import serve
    import os
    import logging
    import re

    from huggingface_hub import login

    # Environment and configuration setup
    logger = logging.getLogger("ray.serve")

    # Function calling utilities
    class FunctionCallParser:
        @staticmethod
        def extract_function_calls(text: str) -> List[Dict[str, Any]]:
            """Extract function calls from generated text."""
            # Pattern to match function calls in the format:
            # <function_call name="function_name">
            # {
            #   "param1": "value1",
            #   "param2": "value2"
            # }
            # </function_call>
            pattern = r'<function_call name="([^"]+)">\s*([\s\S]*?)\s*</function_call>'
            matches = re.finditer(pattern, text)
            
            function_calls = []
            for match in matches:
                function_name = match.group(1)
                args_str = match.group(2).strip()
                
                try:
                    args = json.loads(args_str)
                    function_calls.append({
                        "name": function_name,
                        "arguments": args
                    })
                except json.JSONDecodeError:
                    logger.warning(f"Failed to parse function call arguments: {args_str}")
                    continue
                    
            return function_calls
            
        @staticmethod
        def format_function_response(function_name: str, response: Dict[str, Any]) -> str:
            """Format function response for the model."""
            response_json = json.dumps(response, indent=2)
            return f'<function_response name="{function_name}">\n{response_json}\n</function_response>'

    @serve.deployment(name="mistral-deployment", route_prefix="/vllm",
        ray_actor_options={"num_gpus": 1},
        autoscaling_config={"min_replicas": 1, "max_replicas": 2},
    )
    class VLLMDeployment:
        def __init__(self, **kwargs):
            hf_token = os.getenv("HUGGING_FACE_HUB_TOKEN")
            logger.info(f"token: {hf_token=}")
            if not hf_token:
                raise ValueError("HUGGING_FACE_HUB_TOKEN environment variable is not set")
            login(token=hf_token)
            logger.info("Successfully logged in to Hugging Face Hub")
            
            # Initialize API key for authentication
            self.api_key = os.getenv("LLM_API_KEY", default="sk-1234")
            logger.info("API key authentication initialized")

            args = AsyncEngineArgs(
                model=os.getenv("MODEL_ID", "mistralai/Mistral-7B-Instruct-v0.2"),  # Model identifier from Hugging Face Hub or local path.
                dtype="auto",  # Automatically determine the data type (e.g., float16 or float32) for model weights and computations.
                gpu_memory_utilization=float(os.getenv("GPU_MEMORY_UTILIZATION", "0.8")),  # Percentage of GPU memory to utilize, reserving some for overhead.
                max_model_len=int(os.getenv("MAX_MODEL_LEN", "4096")),  # Maximum sequence length (in tokens) the model can handle, including both input and output tokens.
                max_num_seqs=int(os.getenv("MAX_NUM_SEQ", "512")),  # Maximum number of sequences (requests) to process in parallel.
                max_num_batched_tokens=int(os.getenv("MAX_NUM_BATCHED_TOKENS", "32768")),  # Maximum number of tokens processed in a single batch across all sequences (max_model_len * max_num_seqs).
                trust_remote_code=True,  # Allow execution of untrusted code from the model repository (use with caution).
                enable_chunked_prefill=False,  # Disable chunked prefill to avoid compatibility issues with prefix caching.
                tokenizer_pool_size=4,  # Number of tokenizer instances to handle concurrent requests efficiently.
                tokenizer_pool_type="ray",  # Pool type for tokenizers; 'ray' uses Ray for distributed processing.
                max_parallel_loading_workers=2,  # Number of parallel workers to load the model concurrently.
                pipeline_parallel_size=1,  # Number of pipeline parallelism stages; typically set to 1 unless using model parallelism.
                tensor_parallel_size=1,  # Number of tensor parallelism stages; typically set to 1 unless using model parallelism.
                enable_prefix_caching=True,  # Enable prefix caching to improve performance for similar prompt prefixes.
                enforce_eager=True,
                disable_log_requests=True,
            )

            self.engine = AsyncLLMEngine.from_engine_args(args)
            self.max_model_len = args.max_model_len
            logger.info(f"VLLM Engine initialized with max_model_len: {self.max_model_len}")

        async def stream_results(self, results_generator) -> AsyncGenerator[bytes, None]:
            num_returned = 0
            async for request_output in results_generator:
                text_outputs = [output.text for output in request_output.outputs]
                assert len(text_outputs) == 1
                text_output = text_outputs[0][num_returned:]
                ret = {"text": text_output}
                
                # Check for function calls in the streamed output
                function_calls = FunctionCallParser.extract_function_calls(text_output)
                if function_calls:
                    ret["function_calls"] = function_calls
                    
                yield (json.dumps(ret) + "\n").encode("utf-8")
                num_returned += len(text_output)

        async def may_abort_request(self, request_id) -> None:
            await self.engine.abort(request_id)

        async def __call__(self, request: Request) -> Response:
            try:
                # Check for API key in request headers
                auth_header = request.headers.get("Authorization", "")
                if auth_header.startswith("Bearer "):
                    api_key = auth_header.replace("Bearer ", "")
                    if api_key != self.api_key:
                        return JSONResponse(
                            status_code=401,
                            content={"error": "Invalid API key"}
                        )
                
                request_dict = await request.json()
            except json.JSONDecodeError:
                return JSONResponse(status_code=400, content={"error": "Invalid JSON in request body"})

            context_length = request_dict.pop("context_length", 8192)  # Default to 8k

            # Ensure context length is either 8k or 32k
            if context_length not in [8192, 32768]:
                context_length = 8192  # Default to 8k if invalid
            prompt = request_dict.pop("prompt")
            stream = request_dict.pop("stream", False)
            
            # Extract function definitions if provided
            tools = request_dict.pop("tools", None)
            function_calling = request_dict.pop("function_calling", None)
            
            # If tools are provided, format them for the model
            if tools:
                # Format tools as part of the prompt for models that don't natively support function calling
                tools_str = json.dumps(tools, indent=2)
                prompt = f"""You have access to the following tools:
                {tools_str}

                To use a tool, please use the following format:
                <function_call name="tool_name">
                {{
                    "param1": "value1",
                    "param2": "value2"
                }}
                </function_call>

                {prompt}"""

            # Get model config and tokenizer
            model_config = await self.engine.get_model_config()
            tokenizer = await self.engine.get_tokenizer()

            input_token_ids = tokenizer.encode(prompt)
            input_tokens = len(input_token_ids)
            max_possible_new_tokens = min(context_length, model_config.max_model_len) - input_tokens
            max_new_tokens = min(request_dict.get("max_tokens", 8192), max_possible_new_tokens)

            sampling_params = SamplingParams(
                max_tokens=max_new_tokens,
                temperature=request_dict.get("temperature", 0.7),
                top_p=request_dict.get("top_p", 0.9),
                top_k=request_dict.get("top_k", 50),
                stop=request_dict.get("stop", None),
            )

            request_id = random_uuid()
            logger.info(f"Processing request {request_id} with {input_tokens} input tokens")

            results_generator = self.engine.generate(prompt, sampling_params, request_id)

            if stream:
                background_tasks = BackgroundTasks()
                # Using background_tasks to abort the request
                # if the client disconnects.
                background_tasks.add_task(self.may_abort_request, request_id)
                return StreamingResponse(
                    self.stream_results(results_generator), background=background_tasks
                )

            # Non-streaming case
            final_output = None
            async for request_output in results_generator:
                if await request.is_disconnected():
                    # Abort the request if the client disconnects.
                    await self.engine.abort(request_id)
                    logger.warning(f"Client disconnected for request {request_id}")
                    return Response(status_code=499)
                final_output = request_output

            assert final_output is not None
            prompt = final_output.prompt
            text_outputs = [output.text for output in final_output.outputs]
            
            # Process function calls in the complete response
            response_text = text_outputs[0]
            ret = {"text": response_text}
            
            # Extract function calls if present
            function_calls = FunctionCallParser.extract_function_calls(response_text)
            if function_calls:
                ret["function_calls"] = function_calls
                
            logger.info(f"Completed request {request_id}")
            return Response(content=json.dumps(ret))


    deployment = VLLMDeployment.bind()
---
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: vllm-function-call
spec:
  serviceUnhealthySecondThreshold: 1800 # Config for the health check threshold for service. Default value is 60.
  deploymentUnhealthySecondThreshold: 1800 # Config for the health check threshold for deployments. Default value is 60.
  serveConfigV2: |
    applications:
      - name: mistral
        import_path: "vllm_serve:deployment"
        runtime_env:
          env_vars:
            LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
            MODEL_ID: "Qwen/QwQ-32B-AWQ"
            GPU_MEMORY_UTILIZATION: "0.9"
            MAX_MODEL_LEN: "8192"
            MAX_NUM_SEQ: "4"
            MAX_NUM_BATCHED_TOKENS: "32768"
            # Set to true to enable function calling support
            ENABLE_FUNCTION_CALLING: "true"
        deployments:
          - name: mistral-deployment
            autoscaling_config:
              metrics_interval_s: 0.2
              min_replicas: 1
              max_replicas: 4
              look_back_period_s: 2
              downscale_delay_s: 600
              upscale_delay_s: 30
              target_num_ongoing_requests_per_replica: 20
            graceful_shutdown_timeout_s: 5
            max_concurrent_queries: 100
            ray_actor_options:
              num_cpus: 1
              num_gpus: 1
  rayClusterConfig:
    rayVersion: '2.24.0' # Should match the Ray version in the image of the containers
    enableInTreeAutoscaling: true
    ######################headGroupSpecs#################################
    # Ray head pod template.
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: "0"
      # Pod template
      template:
        spec:
          containers:
          - name: ray-head
            image: public.ecr.aws/data-on-eks/ray2.24.0-py310-vllm-gpu:v1
            imagePullPolicy: IfNotPresent
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "ray stop"]
            ports:
            - containerPort: 6379
              name: gcs
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
            - mountPath: /home/ray/vllm_serve.py
              name: vllm-code-volume
              subPath: vllm.py
            # resources for pulling the larger images
            resources:
              limits:
                cpu: 2
                memory: "12G"
              requests:
                cpu: 2
                memory: "12G"
            env:
            # Ensure to set VLLM_PORT to avoid conflict with Ray serve port 8000
            # We also noticed an error when trying to deploy multiple replicas in single g5 instance. "Error: torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:8004 (errno: 98 - Address already in use)."
            - name: VLLM_PORT
              value: "8004"
            - name: LD_LIBRARY_PATH
              value: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: hf-token
          nodeSelector:
            kubernetes.io/arch: amd64
            karpenter.sh/nodepool: x86-inference
          tolerations:
          - key: "model-inferencing"
            operator: "Equal"
            value: "x86-inference"
            effect: "NoSchedule"
          volumes:
          - name: ray-logs
            emptyDir: {}
          - name: vllm-code-volume
            configMap:
              name: vllm-code
    workerGroupSpecs:
    # The pod replicas in this group typed worker
    - replicas: 1
      minReplicas: 1
      maxReplicas: 4
      groupName: gpu-group
      rayStartParams: {}
      # Pod template
      template:
        spec:
          containers:
          - name: ray-worker
            image: public.ecr.aws/data-on-eks/ray2.24.0-py310-vllm-gpu:v1
            imagePullPolicy: IfNotPresent
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "ray stop"]
            volumeMounts:
            - mountPath: /home/ray/vllm_serve.py
              name: vllm-code-volume
              subPath: vllm.py
            resources:
              limits:
                cpu: 16
                memory: "50Gi"
                nvidia.com/gpu: 1
              requests:
                cpu: 16
                memory: "50Gi"
                nvidia.com/gpu: 1
            env:
            # Ensure to set VLLM_PORT to avoid conflict with Ray serve port 8000
            - name: VLLM_PORT
              value: "8004"
            - name: LD_LIBRARY_PATH
              value: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: hf-token
            - name: LLM_API_KEY
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: llm-api-key
          # Updated nodeSelector to target GPU instances specifically
          nodeSelector:
            kubernetes.io/arch: amd64
            nvidia.com/gpu: present
            karpenter.sh/nodepool: gpu-inference
          # Updated tolerations to match the taint on the GPU nodepool
          tolerations:
          - key: "model-inferencing"
            operator: "Equal"
            value: "gpu-inference"
            effect: "NoSchedule"
          volumes:
          - name: vllm-code-volume
            configMap:
              name: vllm-code
