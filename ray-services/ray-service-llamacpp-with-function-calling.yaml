apiVersion: v1
kind: Secret
metadata:
  name: token
stringData:
  token: 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
  llm_api_key: sk-1234
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-app-code
data:
  app.py: |
    import multiprocessing
    import os
    import logging
    import time
    import json
    from fastapi import FastAPI, HTTPException
    from starlette.requests import Request
    from starlette.responses import StreamingResponse, JSONResponse
    from ray import serve
    from llama_cpp import Llama

    logger = logging.getLogger("ray.serve")
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    app = FastAPI()

    @serve.deployment(
        name="LLamaCPPDeployment", 
        ray_actor_options={"num_cpus": 29}, 
        autoscaling_config={
            "min_replicas": 1, 
            "max_replicas": 10, 
            "initial_replicas": 1, 
            "upscale_delay_s": 5
        }, 
        max_ongoing_requests=100, 
        graceful_shutdown_timeout_s=600
    )
    @serve.ingress(app)
    class LLamaCPPDeployment:
        def __init__(self, parallelism: str):
            os.environ["OMP_NUM_THREADS"] = parallelism
            self.model_id = os.getenv("MODEL_ID", default="SanctumAI/Llama-3.2-1B-Instruct-GGUF")
            self.filename = os.getenv("MODEL_FILENAME", default="*Q4_0.gguf")
            self.n_ctx = int(os.getenv("N_CTX"))
            self.n_threads = int(os.getenv("N_THREADS"))
            self.api_key = os.getenv("LLM_API_KEY", default="sk-1234")
            self.llm = Llama.from_pretrained(
                repo_id=self.model_id,
                filename=self.filename,
                n_ctx=self.n_ctx,
                n_threads=self.n_threads
            )
            logger.info(f"Model {self.model_id} loaded successfully with {self.n_threads} threads")
            print("__init__ Complete")

        @app.post("/v1/chat/completions")
        async def call_llama(self, request: Request):
            try:
                start_time = time.time()
                body = await request.json()
                
                # Check for API key in request headers
                auth_header = request.headers.get("Authorization", "")
                if auth_header.startswith("Bearer "):
                    api_key = auth_header.replace("Bearer ", "")
                    if api_key != self.api_key:
                        return JSONResponse(
                            status_code=401,
                            content={"error": "Invalid API key"}
                        )
                
                # Extract parameters from request
                messages = body.get("messages", [])
                functions = body.get("functions", None)
                function_call = body.get("function_call", "auto")
                temperature = body.get("temperature", 0.7)
                max_tokens = body.get("max_tokens", 512)
                
                # Log request info
                logger.info(f"Received request with {len(messages)} messages")
                if functions:
                    logger.info(f"Request includes {len(functions)} functions")
                
                # Prepare the prompt
                prompt = self._format_messages(messages)
                
                # Add function calling context if functions are provided
                if functions:
                    prompt += "\nAvailable functions:\n"
                    for func in functions:
                        prompt += f"- {func['name']}: {func['description']}\n"
                        if 'parameters' in func:
                            prompt += f"  Parameters: {json.dumps(func['parameters'])}\n"
                    
                    if function_call and function_call != "auto":
                        if function_call == "none":
                            prompt += "\nDo not use any functions for this response.\n"
                        elif isinstance(function_call, dict) and "name" in function_call:
                            prompt += f"\nYou should use the function: {function_call['name']}\n"
                
                prompt += "\nAssistant: "
                
                # Generate response
                logger.info(f"Sending prompt to model (length: {len(prompt)})")
                output = self.llm(
                    prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stop=["User:", "System:"]
                )
                
                response_text = output["choices"][0]["text"]
                logger.info(f"Model generated response (length: {len(response_text)})")
                
                # Process the response for function calling
                function_call_data = None
                if functions:
                    function_call_data = self._extract_function_call(response_text)
                    if function_call_data:
                        # Remove the function call from the response text if it was extracted
                        response_text = response_text.split("```")[0].strip()
                        logger.info(f"Extracted function call: {function_call_data['name']}")
                
                # Prepare the response
                response = {
                    "id": "cmpl-" + os.urandom(12).hex(),
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": self.model_id,
                    "choices": [{
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": response_text if not function_call_data else None
                        },
                        "finish_reason": "stop" if not function_call_data else "function_call"
                    }],
                    "usage": {
                        "prompt_tokens": len(prompt.split()),
                        "completion_tokens": len(response_text.split()),
                        "total_tokens": len(prompt.split()) + len(response_text.split())
                    }
                }
                
                # Add function call to the response if present
                if function_call_data:
                    response["choices"][0]["message"]["function_call"] = {
                        "name": function_call_data["name"],
                        "arguments": json.dumps(function_call_data["arguments"])
                    }
                
                duration = time.time() - start_time
                logger.info(f"Request processed in {duration:.2f} seconds")
                
                return JSONResponse(content=response)
                
            except Exception as e:
                logger.error(f"Error processing request: {str(e)}")
                return JSONResponse(
                    status_code=500,
                    content={"error": str(e)}
                )
        
        def _format_messages(self, messages):
            """Format messages into a prompt string"""
            prompt = ""
            for message in messages:
                role = message.get("role", "")
                content = message.get("content", "")
                
                if role == "user":
                    prompt += f"User: {content}\n"
                elif role == "assistant":
                    prompt += f"Assistant: {content}\n"
                elif role == "system":
                    prompt += f"System: {content}\n"
                elif role == "function":
                    name = message.get("name", "")
                    prompt += f"Function ({name}): {content}\n"
            
            return prompt
        
        def _extract_function_call(self, text):
            """Extract function call from model output"""
            try:
                # Try to find JSON in code blocks
                if "```json" in text and "```" in text:
                    json_start = text.find("```json") + 7
                    json_end = text.find("```", json_start)
                    json_str = text[json_start:json_end].strip()
                    function_data = json.loads(json_str)
                    
                    if "name" in function_data and "arguments" in function_data:
                        # Convert arguments to dict if it's a string
                        if isinstance(function_data["arguments"], str):
                            try:
                                function_data["arguments"] = json.loads(function_data["arguments"])
                            except:
                                pass  # Keep as string if not valid JSON
                        return function_data
                
                # Try to find JSON without code blocks
                elif "{" in text and "}" in text:
                    # Find the first { and the last }
                    json_start = text.find("{")
                    json_end = text.rfind("}") + 1
                    json_str = text[json_start:json_end].strip()
                    
                    try:
                        function_data = json.loads(json_str)
                        if "name" in function_data and "arguments" in function_data:
                            # Convert arguments to dict if it's a string
                            if isinstance(function_data["arguments"], str):
                                try:
                                    function_data["arguments"] = json.loads(function_data["arguments"])
                                except:
                                    pass  # Keep as string if not valid JSON
                            return function_data
                    except:
                        pass
                
                # Check for specific patterns like "I need to call function X with arguments Y"
                function_call_indicators = [
                    "I'll use the function", 
                    "I need to call", 
                    "Let me call",
                    "Calling function",
                    "I will call"
                ]
                
                for indicator in function_call_indicators:
                    if indicator in text:
                        # Try to extract function name and arguments
                        lines = text.split("\n")
                        for line in lines:
                            if indicator in line:
                                # Simple heuristic to extract function name
                                for word in line.split():
                                    if "get_" in word or "_" in word:
                                        potential_name = word.strip(",.()\"'")
                                        # Look for arguments in subsequent lines
                                        arg_start = text.find("{", text.find(potential_name))
                                        if arg_start > -1:
                                            arg_end = self._find_matching_bracket(text, arg_start)
                                            if arg_end > -1:
                                                try:
                                                    args = json.loads(text[arg_start:arg_end+1])
                                                    return {
                                                        "name": potential_name,
                                                        "arguments": args
                                                    }
                                                except:
                                                    pass
            except Exception as e:
                logger.error(f"Error extracting function call: {str(e)}")
            
            return None
        
        def _find_matching_bracket(self, text, start_pos):
            """Find the matching closing bracket for an opening bracket"""
            if text[start_pos] != '{':
                return -1
                
            stack = 1
            for i in range(start_pos + 1, len(text)):
                if text[i] == '{':
                    stack += 1
                elif text[i] == '}':
                    stack -= 1
                    if stack == 0:
                        return i
            
            return -1

        @app.get("/health")
        async def health_check(self):
            return {"status": "healthy", "model": self.model_id}

    host_cpu_count = multiprocessing.cpu_count()

    model = LLamaCPPDeployment.bind("host_cpu_count")
---
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: ray-service-llamacpp
spec:
  serveConfigV2: |
    applications:
    - name: llmcpp-arm
      route_prefix: /
      import_path: app:model
      deployments:
      - name: LLamaCPPDeployment
        ray_actor_options:
          num_cpus: 29
      runtime_env:
        pip: ["llama_cpp_python", "transformers"]
        env_vars:
          LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
          MODEL_ID: "QuantFactory/Phi-3.5-mini-instruct-GGUF" 
          MODEL_FILENAME: "Phi-3.5-mini-instruct.Q4_K_M.gguf"
          N_CTX: "0"
          N_THREADS : "32"
          FORCE_CMAKE: "1"
          CMAKE_ARGS: "-DCMAKE_CXX_FLAGS='-mcpu=native -fopenmp' -DCMAKE_C_FLAGS='-mcpu=native -fopenmp'"
          CMAKE_CXX_COMPILER: "/usr/bin/g++"  
          CMAKE_C_COMPILER: "/usr/bin/gcc"  
          CXX: "/usr/bin/g++"  
          CC: "/usr/bin/gcc" 
          PYTHONPATH: "/home/ray/anaconda3/lib/python3.11/site-packages:$PYTHONPATH" 
  rayClusterConfig:
    rayVersion: '2.33.0'
    enableInTreeAutoscaling: true
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: '0'
      template:
        metadata:
          labels:
            ray-control-plane: "true"
        spec:
          nodeSelector:
            kubernetes.io/arch: arm64
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                    - arm64           
          restartPolicy: Always
          containers:
          - name: ray-head
            image: rayproject/ray:2.39.0-py312-cpu-aarch64
            volumeMounts:
              - name: app-code
                mountPath: /home/ray/app.py
                subPath: app.py
            resources:
              limits:
                cpu: "10"
                memory: "20Gi"
              requests:
                cpu: "10"
                memory: "20Gi"
            ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: token
                  key: token
            - name: LLM_API_KEY
              valueFrom:
                secretKeyRef:
                  name: token
                  key: llm_api_key
            - name: RAY_enable_autoscaler_v2
              value: "1"
            - name: RAY_num_heartbeats_timeout
              value: "300" 
          volumes:
              - name: app-code
                configMap:
                  name: llama-app-code
    workerGroupSpecs:
    - replicas: 1
      minReplicas: 1
      maxReplicas: 10
      groupName: cpu-group
      rayStartParams: 
        num-cpus: "29"
      template:
        spec:
          nodeSelector:
            kubernetes.io/arch: arm64
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                    - arm64           
          restartPolicy: Always
          containers:
          - name: llm
            image: rayproject/ray:2.39.0-py312-cpu-aarch64
            volumeMounts:
              - name: app-code
                mountPath: /home/ray/app.py
                subPath: app.py           
            env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: token
                  key: token
            - name: LLM_API_KEY
              valueFrom:
                secretKeyRef:
                  name: token
                  key: llm_api_key
            - name: CMAKE_ARGS
              value: "-DCMAKE_CXX_FLAGS=-fopenmp"  
            - name: "CC"
              value: "/usr/bin/gcc"   
            - name: "CXX"
              value: "/usr/bin/g++"   
            - name: CMAKE_CXX_COMPILER
              value: "/usr/bin/g++"  
            - name: CMAKE_C_COMPILER
              value: "/usr/bin/gcc" 
            - name: PYTHONPATH
              value: "/home/ray/anaconda3/lib/python3.11/site-packages:$PYTHONPATH" 
            resources:
              limits:
                cpu: "30"
                memory: "55Gi"        
              requests:
                cpu: "30"
                memory: "55Gi"
          volumes:
              - name: app-code
                configMap:
                  name: llama-app-code
